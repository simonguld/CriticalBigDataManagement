{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for data scraping a website\n",
    "\n",
    "\n",
    "$\\textit{Authors:}$ Anton Golles \\& Simon Guldager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "import json\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=408&sort=relevance&term=type+2+diabetes+Rossing+P\n",
      "Failed to fetch article with id:  25905328\n",
      "Failed to fetch article with id:  34279876\n"
     ]
    }
   ],
   "source": [
    "# set the url\n",
    "url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=NUM&sort=relevance&term=KEYWORD\"\n",
    "\n",
    "# We ask the user to provide the keyword and number of results and subsequently replace these elements in the url string\n",
    "keyword = str(input('Please enter the keyword eg. \"type+2+diabetes\"')) \n",
    "num = int(input('Please enter the number of results - Numbers above 500 may cause it to fail'))\n",
    "url = url.replace('NUM', str(num))\n",
    "url = url.replace('KEYWORD', keyword)\n",
    "print(url)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn’t verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn’t support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "webpage = urllib.request.urlopen(url).read()\n",
    "dict_page =json.loads(webpage)\n",
    "idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "\n",
    "# We create a function to delete brackets from titles\n",
    "def strip_brackets(s): \n",
    "    # initialization of string to \"\" \n",
    "    no_bracktes = \"\" \n",
    "    dont_want = ['[',']']\n",
    "    # traverse in the string  \n",
    "    for char in s: \n",
    "        if char not in dont_want:\n",
    "            no_bracktes += char\n",
    "    # return string  \n",
    "    return no_bracktes \n",
    "\n",
    "\n",
    "# Create a function which takes the soup and extracts all needed elements for the bibliography and abstract\n",
    "def get_bibliography(soup):\n",
    "\n",
    "    # This function creates a empty variable for each needed element and subsequently fills in the true value if it exists\n",
    "    article = soup.find('article')\n",
    "    journal = soup.find('journal')\n",
    "\n",
    "    authorlist = article.find('authorlist')\n",
    "\n",
    "    # Extracting list of authors\n",
    "    authors = \"\"\n",
    "    if authorlist:\n",
    "        for i in range(len(authorlist.find_all('lastname'))):\n",
    "            try:\n",
    "                initial = authorlist.find_all('initials')[i].text\n",
    "            except:\n",
    "                initial = ''\n",
    "            authors += initial\n",
    "            authors += '. '\n",
    "            last_name = authorlist.find_all('lastname')[i].text\n",
    "            authors+= last_name\n",
    "            if i == len(authorlist.find_all('lastname'))-2:\n",
    "                authors += ' and '\n",
    "            elif i != len(authorlist.find_all('lastname'))-1:\n",
    "                authors += ', '\n",
    "        authors += \", \"\n",
    "        \n",
    "    # Extracting title of the article\n",
    "    ArticleTitle = ''\n",
    "    if article.find('articletitle'):\n",
    "            ArticleTitle = '\"'\n",
    "            title_str = article.find('articletitle').text\n",
    "            title_str = strip_brackets(title_str)\n",
    "            ArticleTitle += title_str\n",
    "            # If that is in the title, please leave it and put the comma after the quotation marks. - Professor Bishop\n",
    "            if ArticleTitle[-1] == '.':\n",
    "                ArticleTitle += '\", '\n",
    "            else:\n",
    "                ArticleTitle += ',\" '\n",
    "    \n",
    "    # Extracting date of the article\n",
    "    JournalIssue = journal.find('journalissue')\n",
    "    \n",
    "    month = JournalIssue.find('month')\n",
    "    date = ''\n",
    "    if month:\n",
    "        month = JournalIssue.find('month').text\n",
    "        if len(month)<3:\n",
    "            month_int = int(str(month))\n",
    "            month = calendar.month_abbr[month_int]\n",
    "\n",
    "        year = JournalIssue.find('year').text\n",
    "        date += month\n",
    "        date += '. '\n",
    "        date += year\n",
    "    elif JournalIssue.find('year'):\n",
    "        date+= JournalIssue.find('year').text   \n",
    "    else: ''\n",
    "\n",
    "    # Extracting abstract      \n",
    "    abstract = ''\n",
    "    if article.find('abstracttext'):\n",
    "        abstract += '\"'\n",
    "        abstract += article.find('abstracttext').text\n",
    "        abstract += '\"'\n",
    "        \n",
    "    # Extracting list of keywords  \n",
    "    keywordlist  = soup.find('keywordlist')\n",
    "    keywords = \"\"\n",
    "    if keywordlist:\n",
    "        for i in range(len(keywordlist.find_all('keyword'))):\n",
    "            keyword = keywordlist.find_all('keyword')[i].text\n",
    "            keywords+= keyword\n",
    "            keywords += \", \"\n",
    "            \n",
    "    # Extracting list of affiliations - NB! Dublicates may occur, handle this in later cleaning process  \n",
    "    affiliationlist  = soup.find_all('affiliation') \n",
    "    affiliations = \"\"\n",
    "    if affiliationlist:\n",
    "        for i in range(len(soup.find_all('affiliation'))):\n",
    "            affiliation = soup.find_all('affiliation')[i].text\n",
    "            affiliations+= '\"'\n",
    "            affiliations+= affiliation\n",
    "            affiliations+= '\", '\n",
    "    \n",
    "    result = []\n",
    "    result.append(authors)\n",
    "    result.append(ArticleTitle)\n",
    "    result.append(date)\n",
    "    result.append(abstract)\n",
    "    result.append(keywords)\n",
    "    result.append(affiliations)\n",
    "    return result\n",
    "\n",
    "articles_list = []\n",
    "nsucces = 0\n",
    "\n",
    "# We loop over each element in the idlist to get the soup and feed it into our function\n",
    "for link in idlist:\n",
    "    url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "    url = url.replace('idlist', link)\n",
    "\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        # Legacy Python that doesn’t verify HTTPS certificates by default\n",
    "        pass\n",
    "    else:\n",
    "        # Handle target environment that doesn’t support HTTPS verification\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        article = get_bibliography(soup)\n",
    "        articles_list.append(article)\n",
    "        nsucces += 1\n",
    "    except:\n",
    "        print('Failed to fetch article with id: ', link)\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(articles_list)\n",
    "df.columns = ['Authors', 'ArticleTitle', 'Date', 'Abstract','Keywords','Affiliations']\n",
    "file_name = keyword + '_' + str(nsucces) + '.csv'\n",
    "df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Authors\n",
       "HH. Parving and P. Rossing,                                                                                                                                                                                                                                                                             5\n",
       "P. Rossing,                                                                                                                                                                                                                                                                                             4\n",
       "JB. Buse, DJ. Wexler, A. Tsapas, P. Rossing, G. Mingrone, C. Mathieu, DA. D'Alessio and MJ. Davies,                                                                                                                                                                                                     4\n",
       "F. Persson, JB. Lewis, EJ. Lewis, P. Rossing, NK. Hollenberg and HH. Parving,                                                                                                                                                                                                                           3\n",
       "MJ. Davies, DA. D'Alessio, J. Fradkin, WN. Kernan, C. Mathieu, G. Mingrone, P. Rossing, A. Tsapas, DJ. Wexler and JB. Buse,                                                                                                                                                                             3\n",
       "P. Rossing, E. Burgess, R. Agarwal, SD. Anker, G. Filippatos, B. Pitt, LM. Ruilope, P. Gillard, RJ. MacIsaac, J. Wainstein, A. Joseph, M. Brinker, L. Roessig, C. Scott and GL. Bakris,                                                                                                                 2\n",
       "M. Vaduganathan, G. Filippatos, BL. Claggett, AS. Desai, PS. Jhund, A. Henderson, M. Brinker, P. Kolkhof, P. Schloemer, J. Lay-Flurrie, P. Viswanathan, CSP. Lam, M. Senni, SJ. Shah, AA. Voors, F. Zannad, P. Rossing, LM. Ruilope, SD. Anker, B. Pitt, R. Agarwal, JJV. McMurray and SD. Solomon,     2\n",
       "F. Persson, P. Rossing, H. Reinhard, T. Juhl, CD. Stehouwer, C. Schalkwijk, AH. Danser, F. Boomsma, E. Frandsen and HH. Parving,                                                                                                                                                                        2\n",
       "J. Jensen, M. Schou, C. Kistorp, J. Faber, TW. Hansen, MT. Jensen, HU. Andersen, P. Rossing, T. Vilsbøll and PG. Jørgensen,                                                                                                                                                                             2\n",
       "S. Eder, J. Leierer, J. Kerschbaum, L. Rosivall, A. Wiecek, D. de Zeeuw, PB. Mark, G. Heinze, P. Rossing, HL. Heerspink and G. Mayer,                                                                                                                                                                   2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Authors'].value_counts()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
