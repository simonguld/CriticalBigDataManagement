{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for data scraping a website\n",
    "\n",
    "\n",
    "$\\textit{Author:}$ Anton Golles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "import json\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=400&sort=relevance&term=type+2+diabetes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simon Andersen\\miniconda3\\envs\\BigData\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set the url\n",
    "url=\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=NUM&sort=relevance&term=KEYWORD\"\n",
    "\n",
    "# We ask the user to provide the keyword and number of results and subsequently replace these elements in the url string\n",
    "keyword = str(input('Please enter the keyword eg. \"type+2+diabetes\"')) \n",
    "num = int(input('Please enter the number of results - Numbers above 500 may cause it to fail'))\n",
    "url = url.replace('NUM', str(num))\n",
    "url = url.replace('KEYWORD', keyword)\n",
    "print(url)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn’t verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn’t support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "webpage = urllib.request.urlopen(url).read()\n",
    "dict_page =json.loads(webpage)\n",
    "idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "\n",
    "# We create a function to delete brackets from titles\n",
    "def strip_brackets(s): \n",
    "    # initialization of string to \"\" \n",
    "    no_bracktes = \"\" \n",
    "    dont_want = ['[',']']\n",
    "    # traverse in the string  \n",
    "    for char in s: \n",
    "        if char not in dont_want:\n",
    "            no_bracktes += char\n",
    "    # return string  \n",
    "    return no_bracktes \n",
    "\n",
    "\n",
    "# Create a function which takes the soup and extracts all needed elements for the bibliography and abstract\n",
    "def get_bibliography(soup):\n",
    "\n",
    "    # This function creates a empty variable for each needed element and subsequently fills in the true value if it exists\n",
    "    article = soup.find('article')\n",
    "    journal = soup.find('journal')\n",
    "\n",
    "    authorlist = article.find('authorlist')\n",
    "\n",
    "    # Extracting list of authors\n",
    "    authors = \"\"\n",
    "    if authorlist:\n",
    "        for i in range(len(authorlist.find_all('lastname'))):\n",
    "            try:\n",
    "                initial = authorlist.find_all('initials')[i].text\n",
    "            except:\n",
    "                initial = ''\n",
    "            authors += initial\n",
    "            authors += '. '\n",
    "            last_name = authorlist.find_all('lastname')[i].text\n",
    "            authors+= last_name\n",
    "            if i == len(authorlist.find_all('lastname'))-2:\n",
    "                authors += ' and '\n",
    "            elif i != len(authorlist.find_all('lastname'))-1:\n",
    "                authors += ', '\n",
    "        authors += \", \"\n",
    "        \n",
    "    # Extracting title of the article\n",
    "    ArticleTitle = ''\n",
    "    if article.find('articletitle'):\n",
    "            ArticleTitle = '\"'\n",
    "            title_str = article.find('articletitle').text\n",
    "            title_str = strip_brackets(title_str)\n",
    "            ArticleTitle += title_str\n",
    "            # If that is in the title, please leave it and put the comma after the quotation marks. - Professor Bishop\n",
    "            if ArticleTitle[-1] == '.':\n",
    "                ArticleTitle += '\", '\n",
    "            else:\n",
    "                ArticleTitle += ',\" '\n",
    "    \n",
    "    # Extracting date of the article\n",
    "    JournalIssue = journal.find('journalissue')\n",
    "    \n",
    "    month = JournalIssue.find('month')\n",
    "    date = ''\n",
    "    if month:\n",
    "        month = JournalIssue.find('month').text\n",
    "        if len(month)<3:\n",
    "            month_int = int(str(month))\n",
    "            month = calendar.month_abbr[month_int]\n",
    "\n",
    "        year = JournalIssue.find('year').text\n",
    "        date += month\n",
    "        date += '. '\n",
    "        date += year\n",
    "    elif JournalIssue.find('year'):\n",
    "        date+= JournalIssue.find('year').text   \n",
    "    else: ''\n",
    "\n",
    "    # Extracting abstract      \n",
    "    abstract = ''\n",
    "    if article.find('abstracttext'):\n",
    "        abstract += '\"'\n",
    "        abstract += article.find('abstracttext').text\n",
    "        abstract += '\"'\n",
    "        \n",
    "    # Extracting list of keywords  \n",
    "    keywordlist  = soup.find('keywordlist')\n",
    "    keywords = \"\"\n",
    "    if keywordlist:\n",
    "        for i in range(len(keywordlist.find_all('keyword'))):\n",
    "            keyword = keywordlist.find_all('keyword')[i].text\n",
    "            keywords+= keyword\n",
    "            keywords += \", \"\n",
    "            \n",
    "    # Extracting list of affiliations - NB! Dublicates may occur, handle this in later cleaning process  \n",
    "    affiliationlist  = soup.find_all('affiliation') \n",
    "    affiliations = \"\"\n",
    "    if affiliationlist:\n",
    "        for i in range(len(soup.find_all('affiliation'))):\n",
    "            affiliation = soup.find_all('affiliation')[i].text\n",
    "            affiliations+= '\"'\n",
    "            affiliations+= affiliation\n",
    "            affiliations+= '\", '\n",
    "    \n",
    "    result = []\n",
    "    result.append(authors)\n",
    "    result.append(ArticleTitle)\n",
    "    result.append(date)\n",
    "    result.append(abstract)\n",
    "    result.append(keywords)\n",
    "    result.append(affiliations)\n",
    "    return result\n",
    "\n",
    "articles_list = []\n",
    "\n",
    "# We loop over each element in the idlist to get the soup and feed it into our function\n",
    "for link in idlist:\n",
    "    url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "    url = url.replace('idlist', link)\n",
    "\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        # Legacy Python that doesn’t verify HTTPS certificates by default\n",
    "        pass\n",
    "    else:\n",
    "        # Handle target environment that doesn’t support HTTPS verification\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    article = get_bibliography(soup)\n",
    "    articles_list.append(article)\n",
    "\n",
    "df = pd.DataFrame(articles_list)\n",
    "df.columns = ['Authors', 'ArticleTitle', 'Date', 'Abstract','Keywords','Affiliations']\n",
    "file_name = keyword + '_' + str(num) + '.csv'\n",
    "df.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
