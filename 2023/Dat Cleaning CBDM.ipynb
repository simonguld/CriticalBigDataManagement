{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03a6c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and manipulation library\n",
    "import pandas as pd\n",
    "# Data visualization library based on matplotlib\n",
    "import seaborn as sns\n",
    "# Basic plotting library in Python\n",
    "import matplotlib.pyplot as plt\n",
    "# Cross-validation function to evaluate model performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Library for numerical operations in Python\n",
    "import numpy as np\n",
    "# Preprocessing tool to standardize features (mean=0, variance=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Principal Component Analysis (PCA) for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "# Visualization tool for the elbow method to determine the optimal number of clusters\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "# Calculates the Silhouette Score which measures the quality of clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "# KMeans clustering algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "# Imputation functions\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "# Library for interactive plotting \n",
    "import plotly\n",
    "# Module for creating various chart types (like scatter plots)\n",
    "import plotly.graph_objects as go\n",
    "# Simplified module for creating visualizations in Plotly\n",
    "import plotly.express as px\n",
    "# k-Nearest Neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Splits data into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Generates a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bed67",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing\n",
    "\n",
    "In the first part of this notebook, the focus will be on preparing and cleaning the data for subsequent analysis. The main principles will be covered for some of the features, and it is up to you to do it for the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a66187f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/descriptives.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ab7f8",
   "metadata": {},
   "source": [
    "Step 1: Discard uniformative features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "443a07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "\n",
    "# Since patient_id, conversation_id and name are unique to each patient and conversation,\n",
    "#  they are not useful for clustering/classification. They will become important later though\n",
    "\n",
    "#cols_to_drop.append('patient_id')\n",
    "cols_to_drop.append('name')\n",
    "cols_to_drop.append('conversation_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get a sense of which features offer meaningful information, we print out the frequency of each\n",
    "#value for each feature\n",
    "\n",
    "Nrows = df.shape[0]\n",
    "# Go through each column except for the ones we want to drop\n",
    "for col in df.columns.drop(cols_to_drop):\n",
    "    # get unique values in column\n",
    "    unique = df[col].unique();\n",
    "    print(\"\\n\", col,\":\")\n",
    "    # calculate frequency of each value\n",
    "    for val in unique:\n",
    "        if val not in ['nan']:\n",
    "            N = len(df[df[col] == val])\n",
    "            print(\"Value, N observations, frequency :\", val,\" ,\", N, \" ,\", np.round(N/Nrows,3))\n",
    "\n",
    "    print(\"No. of NANs \", len(df[df[col].isna()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "731573e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the above, we can see that the following features are not useful for clustering/classification,\n",
    "# simply because they have too many missing values. \n",
    "cols_to_drop.extend(['average_sleep_duration', 'systolic', 'diastolic', 'health_related_activities'])\n",
    "\n",
    "# Similarly, since current_country_of_residence is the same for all patients, it is not useful for clustering/classification\n",
    "cols_to_drop.append('current_country_of_residence')\n",
    "\n",
    "# Since all non-nan values for drug_usage are False, it is not useful for clustering/classification\n",
    "cols_to_drop.append('drug_usage')\n",
    "\n",
    "# Since ethnicity and country_of_birth are almost equivalent, we can drop one of them\n",
    "# Finally, since state_code and country_code contain the same information, we can drop one of them\n",
    "cols_to_drop.append('ethnicity')\n",
    "cols_to_drop.append('state_code')\n",
    "\n",
    "# Drop the columns we don't need\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc3103",
   "metadata": {},
   "source": [
    "Step 2: Convert features with potential to be numeric to numeric features. In our case, this amounts to transforming the average_blood_pressure feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3461f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value, N observations, frequency : nan  , 0  , 0.0\n",
      "Value, N observations, frequency : 120/80  , 229  , 0.763\n",
      "Value, N observations, frequency : 120/80 mmHg  , 2  , 0.007\n",
      "Value, N observations, frequency : 130/85  , 5  , 0.017\n",
      "Value, N observations, frequency : 130/80  , 7  , 0.023\n",
      "No. of NANs  57\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the average_blood_presure feature\n",
    "unique = df['average_blood_pressure'].unique();\n",
    "# calculate frequency of each value\n",
    "for val in unique:\n",
    "    if val not in ['nan']:\n",
    "        N = len(df[df['average_blood_pressure'] == val])\n",
    "        print(\"Value, N observations, frequency :\", val,\" ,\", N, \" ,\", np.round(N/Nrows,3))\n",
    "\n",
    "print(\"No. of NANs \", len(df[df['average_blood_pressure'].isna()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "552734b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert this feature into a numerical one (well, two), we will split it into two features: systolic and diastolic\n",
    "\n",
    "# Get rid of the unit\n",
    "df['average_blood_pressure'] = df['average_blood_pressure'].str.replace(' mmHg', '')\n",
    "\n",
    "# Split the feature into two\n",
    "df[['systolic','diastolic']] = df['average_blood_pressure'].str.split('/',expand=True)\n",
    "\n",
    "# Convert the two new features to numeric\n",
    "df['systolic'] = pd.to_numeric(df['systolic'])\n",
    "df['diastolic'] = pd.to_numeric(df['diastolic'])\n",
    "\n",
    "# Drop the now redundant average_blood_pressure feature\n",
    "df.drop('average_blood_pressure', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fa6bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of units and convert to numeric for the remaining relevant features\n",
    "\n",
    "# remove ' cm' suffix, then convert to float\n",
    "df['height'] = df['height'].str.replace(' cm', '')\n",
    "df['height'] = df['height'].astype(float)\n",
    "\n",
    "# Remove ' kg' suffix from 'weight' and convert to float\n",
    "df['weight'] = df['weight'].str.replace(' kg', '')\n",
    "df['weight'] = df['weight'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a270b2",
   "metadata": {},
   "source": [
    "Step 3: Fill out (=impute) missing numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26d29d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age :\n",
      "No. of NANs  0\n",
      "weight :\n",
      "No. of NANs  1\n",
      "height :\n",
      "No. of NANs  1\n",
      "bmi :\n",
      "No. of NANs  1\n",
      "average_daily_step_count :\n",
      "No. of NANs  0\n",
      "resting_heart_rate :\n",
      "No. of NANs  0\n",
      "heart_rate_variability :\n",
      "No. of NANs  0\n",
      "average_blood_glucose :\n",
      "No. of NANs  0\n",
      "average_fasting_glucose :\n",
      "No. of NANs  1\n",
      "number_of_children :\n",
      "No. of NANs  88\n",
      "screen_time_per_day :\n",
      "No. of NANs  1\n",
      "average_sleep_duration_hours :\n",
      "No. of NANs  85\n",
      "systolic :\n",
      "No. of NANs  57\n",
      "diastolic :\n",
      "No. of NANs  57\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the number of Nans for each numeric column\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "Nrows = df.shape[0]\n",
    "# Go through each column except for the ones we want to drop\n",
    "for col in num_cols:\n",
    "    # get unique values in column\n",
    "    unique = df[col].unique();\n",
    "    print(col,\":\")\n",
    "    print(\"No. of NANs \", len(df[df[col].isna()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad0ce126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that some features have as much as 88 (= 30 %) missing values, which is much more than we would like. One could certainly\n",
    "# argue that these features should be dropped. We choose to keep them in the following, however.\n",
    "\n",
    "# Too impute missing values, we introduce the following functions\n",
    "\n",
    "# This imputer only simply replace the missing values with mean, median or most frequent value of that feature\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean') \n",
    "\n",
    "# This imputer considers all numeric features and assigns to each missing value the feature mean of the n nearest neighbors\n",
    "knn_imputer = KNNImputer(missing_values = np.nan, n_neighbors = 5)\n",
    "\n",
    "# We will use the knn imputer, since it considers all numeric features to find the most similar neighbors:\n",
    "df.loc[:, num_cols] = knn_imputer.fit_transform(df.loc[:, num_cols] )\n",
    "\n",
    "# Finally, since non-integer number of childrens are not very meaningful, we'll round the estimated no of children\n",
    "df['number_of_children'] = df['number_of_children'].round(decimals = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065be40",
   "metadata": {},
   "source": [
    "Step 4: Convert categorical features to numeric ones\n",
    "* Convert features with two values to boolean features\n",
    "* Label encode features with order\n",
    "* (Cleverly) group and one hot encode features without order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc4cef11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Moderate', 'Low', nan], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stress_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5128c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features with two values to boolean\n",
    "\n",
    "bool_cols = ['gender', 'stress_level', 'physical_activity', 'alcohol_consumption']\n",
    "\n",
    "alcohol_consumption_order = {'Low': 0, 'Moderate': 1}\n",
    "df['alcohol_consumption'] = df['alcohol_consumption'].map(alcohol_consumption_order)\n",
    "\n",
    "gender_order = {'Male': 0, 'Female': 1}\n",
    "df['gender'] = df['gender'].map(gender_order)\n",
    "\n",
    "\n",
    "stress_level_order = {'Low': 0, 'Moderate': 1}\n",
    "df['stress_level'] = df['stress_level'].map(stress_level_order)\n",
    "\n",
    "# Group low and moderate together since only 1 person reported low physical activity\n",
    "physical_activity_order = {'Low': 0, 'Moderate': 0, 'High': 1}\n",
    "df['physical_activity'] = df['physical_activity'].map(physical_activity_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99029b7f",
   "metadata": {},
   "source": [
    "#### Ranking ordered features (ordinal features) using label encoding:\n",
    "\n",
    "Since one-hot encoding can increase the no. of features dramatically,\n",
    "we will use label encoding for ordinal features. Sometimes, the order is \n",
    "very clear (e.g. low, mid, high, very high), and sometimes we have to be\n",
    "a bit creative.\n",
    "\n",
    "For example, we could rank country_of_birth by GDP per capita, and we could\n",
    "rank state_name by average BMI. We could also rank housing by the average\n",
    "size of each housing type, and tenure from renting to owning to owning without mortgage.\n",
    "\n",
    "In the following, we'll convert the activity and state_name features by finding a metric for the order and then using label encoding. We'll start by reformatting the activities feature to make them easier to work with, after which we'll transform category to a numerical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68f30684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['Running', 'Walking', 'Yoga', 'Swimming', 'Da...\n",
      "1    ['Walking', 'Swimming', 'Running', 'Yoga', 'Da...\n",
      "2    ['Running', 'Walking', 'Swimming', 'Dancing', ...\n",
      "3    ['Walking', 'Yoga', 'Swimming', 'Dancing', 'Cy...\n",
      "4    ['Running', 'Walking', 'Yoga', 'Swimming', 'Da...\n",
      "5                   ['Running', 'Walking', 'Swimming']\n",
      "Name: activities, dtype: object\n",
      "\n",
      "Unique activities:  ['Running', 'Walking', 'Yoga', 'Swimming', 'Dancing', 'Cycling', 'Weightlifting', 'Hiking', 'Pilates', 'Tai Chi', 'Aerobics', 'Strength Training']\n"
     ]
    }
   ],
   "source": [
    "## Reformatting the activity feature ##\n",
    "\n",
    "unique_list = []\n",
    "# List of extra stuff we want to drop\n",
    "discard_list = ['\"activities\":', ' (sprints)', '(jazz)', '(hip-hop)', \\\n",
    "                '(marathons, sprints, trail running)', '(ballet, hip-hop, jazz, ballroom)']\n",
    "\n",
    "# Go through each row in the activities column\n",
    "for i, x in enumerate(df.loc[:,'activities']):\n",
    "    x_cleaned = []\n",
    "    # Avoid nans\n",
    "    if type(x) is float:\n",
    "        continue\n",
    "    # Remove the extra stuff\n",
    "    for el in discard_list:\n",
    "        x = x.replace(el,'')\n",
    "    # Split the string into a list of activities\n",
    "    for el in list(x.split(',')):\n",
    "        # Remove extra characters\n",
    "        el = el.strip(\"['\").strip(\"']\").strip('{\"').strip('}').strip('\"').strip()\n",
    "        # Add to activity to list of activities\n",
    "        x_cleaned.append(el)\n",
    "        # Add to list of unique activities if not already there\n",
    "        if el not in unique_list:\n",
    "            unique_list.append(el)\n",
    "\n",
    "    # Replace the original string with the cleaned list\n",
    "    df.loc[i,'activities'] = str(x_cleaned)\n",
    "\n",
    "\n",
    "# Let's take a look at some of the reformatted rows\n",
    "print(df.loc[:5,'activities'])\n",
    "\n",
    "print(\"\\nUnique activities: \", unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b57c4f",
   "metadata": {},
   "source": [
    "The goal is to translate the activities into a measure of peoples physical fitness/health.\n",
    "We will very naively assume that the number of listed activites correspond to the physical activity level\n",
    "of each patient. \n",
    "There are many other ways of transforming these categories. One could e.g. map each activity to the average calorie burning rate and then add the burning rates for each row. Feel free to play around and try different things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b40d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map each activity list to the number of activities on that list\n",
    "for i, x in enumerate(df.loc[:,'activities']):\n",
    "    if type(x) is not float:\n",
    "        df.loc[i,'activities'] = len(list(x.split(',')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da543190",
   "metadata": {},
   "source": [
    "As another example, let's introduce order into the state name feature and then use label encoding. In particular, let's map the state_names, which are not necessarily very informative as is, onto the obesity prevalences (% of obese people) of the corresponding states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cba6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, let us map the state_name feauture onto the prevalence of obsese people (BMI > 30) in the given stats\n",
    "# The obesity prevalence data is from 2021 and can be found and downloaded on\n",
    "#  https://www.cdc.gov/obesity/data/prevalence-maps.html\n",
    "\n",
    "obesity_df = pd.read_csv('data/Obesity-prevalence-by-state-2021.csv')\n",
    "obesity_order = {}\n",
    "\n",
    "# Make obesity order dictionary\n",
    "for row in range(len(obesity_df)):\n",
    "    try:\n",
    "        obesity_order.update({f'{obesity_df.loc[row,\"State\"]}': float(obesity_df.loc[row,'Prevalence'])})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "del_list = []\n",
    "for key in obesity_order.keys():\n",
    "    if key not in df['state_name'].unique():\n",
    "        del_list.append(key)\n",
    "   \n",
    "for key in del_list:\n",
    "    del obesity_order[key]\n",
    "\n",
    "df['state_name'] = df['state_name'].map(obesity_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02931381",
   "metadata": {},
   "source": [
    "In the following, we'll show two examples of how to group and transform features using one-hot (or dummy) encoding. One-hot encoding has the advantage\n",
    "that is does not introduce any order between features like label encoding does, but the drawback that K different categories of a feature will be transformed into K or K-1 new features, and so the total number of features can become very large, which can hurt performance. For this reasons, if a feature has many categories, these categories are often grouped in straightforward (simply group rare categories) or clever (use domain knowledge to make groups).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "372db4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " mother :\n",
      "Value, N observations, frequency : Hypertension  , 7  , 0.023\n",
      "Value, N observations, frequency : No known health issues  , 90  , 0.3\n",
      "Value, N observations, frequency : No known medical conditions  , 170  , 0.567\n",
      "Value, N observations, frequency : No history of diabetes  , 6  , 0.02\n",
      "Value, N observations, frequency : No known health conditions  , 2  , 0.007\n",
      "Value, N observations, frequency : Type 2 diabetes  , 13  , 0.043\n",
      "Value, N observations, frequency : No significant medical history  , 1  , 0.003\n",
      "Value, N observations, frequency : No history of hypertension  , 2  , 0.007\n",
      "Value, N observations, frequency : No history of heart disease  , 1  , 0.003\n",
      "Value, N observations, frequency : Type 2 Diabetes  , 3  , 0.01\n",
      "Value, N observations, frequency : No history of CVD  , 3  , 0.01\n",
      "Value, N observations, frequency : No history of heart disease or diabetes  , 1  , 0.003\n",
      "Value, N observations, frequency : No history of cardiovascular diseases  , 1  , 0.003\n",
      "No. of NANs  0\n",
      "\n",
      "values of mother after grouping:  ['Hypertension' 'No known health issues' 'Type 2 diabetes']\n"
     ]
    }
   ],
   "source": [
    "## Group the observations of the mother feature\n",
    "\n",
    "# Print out each category and its frequency \n",
    "for col in ['mother']:\n",
    "    # get unique values in column\n",
    "    unique = df[col].unique();\n",
    "    print(\"\\n\", col,\":\")\n",
    "    # calculate frequency of each value\n",
    "    for val in unique:\n",
    "        if val not in ['nan']:\n",
    "            N = len(df[df[col] == val])\n",
    "            print(\"Value, N observations, frequency :\", val,\" ,\", N, \" ,\", np.round(N/Nrows,3))\n",
    "\n",
    "    print(\"No. of NANs \", len(df[df[col].isna()]))\n",
    "\n",
    "# Group different spellings\n",
    "mother_map = {'Type 2 Diabetes': 'Type 2 diabetes', }\n",
    "df['mother'].replace({'Type 2 Diabetes': 'Type 2 diabetes'}, inplace=True)\n",
    "\n",
    "# Group all values except for 'Type 2 diabetes' and 'Hypertension' into 'No known health issues'\n",
    "mask = (df['mother'] == 'Type 2 diabetes') | ( df['mother'] == 'Hypertension')\n",
    "df.loc[~mask, 'mother'] = 'No known health issues'\n",
    "\n",
    "print(\"\\nvalues of mother after grouping: \", df['mother'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8baec0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " diet_type :\n",
      "Value, N observations, frequency : Mediterranean  , 47  , 0.157\n",
      "Value, N observations, frequency : Balanced  , 177  , 0.59\n",
      "Value, N observations, frequency : Gluten-free  , 15  , 0.05\n",
      "Value, N observations, frequency : Keto  , 1  , 0.003\n",
      "Value, N observations, frequency : Diabetic-friendly  , 6  , 0.02\n",
      "Value, N observations, frequency : Omnivore  , 7  , 0.023\n",
      "Value, N observations, frequency : Low-carb  , 8  , 0.027\n",
      "Value, N observations, frequency : Low FODMAP  , 1  , 0.003\n",
      "Value, N observations, frequency : Crohn's disease-friendly diet  , 1  , 0.003\n",
      "Value, N observations, frequency : Low Protein  , 1  , 0.003\n",
      "Value, N observations, frequency : Vegetarian  , 11  , 0.037\n",
      "Value, N observations, frequency : Low-FODMAP  , 2  , 0.007\n",
      "Value, N observations, frequency : Anti-inflammatory diet  , 1  , 0.003\n",
      "Value, N observations, frequency : Gestational  , 1  , 0.003\n",
      "Value, N observations, frequency : Plant-based  , 2  , 0.007\n",
      "Value, N observations, frequency : Salt-restricted  , 1  , 0.003\n",
      "Value, N observations, frequency : Vegan  , 2  , 0.007\n",
      "Value, N observations, frequency : Low Glycemic Index  , 2  , 0.007\n",
      "Value, N observations, frequency : Intermittent Fasting  , 2  , 0.007\n",
      "Value, N observations, frequency : Multiple Sclerosis Diet  , 1  , 0.003\n",
      "Value, N observations, frequency : Ayurvedic  , 1  , 0.003\n",
      "Value, N observations, frequency : Anti-Inflammatory Diet  , 1  , 0.003\n",
      "Value, N observations, frequency : High Protein  , 1  , 0.003\n",
      "Value, N observations, frequency : Halal  , 3  , 0.01\n",
      "Value, N observations, frequency : Low-sodium  , 1  , 0.003\n",
      "Value, N observations, frequency : Balanced Diet  , 2  , 0.007\n",
      "Value, N observations, frequency : Anti-inflammatory  , 2  , 0.007\n",
      "No. of NANs  0\n",
      "\n",
      "values of diet_type after grouping:  ['Mediterranean' 'Balanced' 'Other' 'Low-carb' 'Plant-based']\n"
     ]
    }
   ],
   "source": [
    "## Group similar (and also rare) categories of the diet_type features\n",
    "\n",
    "# Go through each column except for the ones we want to drop\n",
    "for col in ['diet_type']:\n",
    "    # get unique values in column\n",
    "    unique = df[col].unique();\n",
    "    print(\"\\n\", col,\":\")\n",
    "    # calculate frequency of each value\n",
    "    for val in unique:\n",
    "        if val not in ['nan']:\n",
    "            N = len(df[df[col] == val])\n",
    "            print(\"Value, N observations, frequency :\", val,\" ,\", N, \" ,\", np.round(N/Nrows,3))\n",
    "\n",
    "    print(\"No. of NANs \", len(df[df[col].isna()]))\n",
    "\n",
    "# Group different spellings\n",
    "df['diet_type'].replace({'Balanced Diet': 'Balanced'}, inplace=True)\n",
    "\n",
    "# Group plant based diets together\n",
    "df['diet_type'].replace({'Vegan': 'Plant-based', 'Vegetarian': 'Plant-based'}, inplace=True)\n",
    "\n",
    "# Group the remaining values into 'Other'\n",
    "mask = (df['diet_type'] == 'Balanced') | ( df['diet_type'] == 'Plant-based') | ( df['diet_type'] == 'Mediterranean') | ( df['diet_type'] == 'Low-carb')\n",
    "df.loc[~mask, 'diet_type'] = 'Other'\n",
    "\n",
    "print(\"\\nvalues of diet_type after grouping: \", df['diet_type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5733a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need this function to do one-hot (or dummy) encoding\n",
    "def do_dummy_encoding(dataframe, target_columns, prefix_list, drop_first = False):\n",
    "    \"\"\"\n",
    "    For each cateogrical column with categories (l1,...,ln), this function transform the category into n ( if drop_first = False)\n",
    "    or n-1 columns, where [0,0,...,0] represents l1,\n",
    "    [1,0,...,0] rep. l2, [0,1,0,...,0] rep l3 etc\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    # Build dummy index dataframe\n",
    "    dummy_columns = pd.get_dummies(data[target_columns], prefix = prefix_list, drop_first=drop_first)\n",
    "\n",
    "    ## Append new columns to dataframe\n",
    "    data = pd.concat([data, dummy_columns], axis = 1)\n",
    "\n",
    "    ## Drop original columns\n",
    "    data.drop(columns = target_columns, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19d8de",
   "metadata": {},
   "source": [
    "Having grouped the categories of mother and diet_type into fewer categories, let us transform the features using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20652db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 41 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   patient_id                     271 non-null    object \n",
      " 1   gender                         300 non-null    int64  \n",
      " 2   age                            300 non-null    int64  \n",
      " 3   country_of_birth               300 non-null    object \n",
      " 4   weight                         300 non-null    float64\n",
      " 5   height                         300 non-null    float64\n",
      " 6   bmi                            300 non-null    float64\n",
      " 7   average_daily_step_count       300 non-null    int64  \n",
      " 8   resting_heart_rate             300 non-null    int64  \n",
      " 9   heart_rate_variability         300 non-null    float64\n",
      " 10  average_blood_glucose          300 non-null    float64\n",
      " 11  average_fasting_glucose        300 non-null    float64\n",
      " 12  specific_preferences           292 non-null    object \n",
      " 13  challenges                     275 non-null    object \n",
      " 14  father                         300 non-null    object \n",
      " 15  current_city_of_residence      300 non-null    object \n",
      " 16  state_name                     270 non-null    float64\n",
      " 17  housing_type                   300 non-null    object \n",
      " 18  housing_tenure                 300 non-null    object \n",
      " 19  marital_status                 300 non-null    object \n",
      " 20  has_children                   300 non-null    bool   \n",
      " 21  number_of_children             300 non-null    float64\n",
      " 22  living_alone                   300 non-null    bool   \n",
      " 23  sexual_orientation             300 non-null    object \n",
      " 24  activities                     297 non-null    object \n",
      " 25  alcohol_consumption            299 non-null    float64\n",
      " 26  stress_level                   299 non-null    float64\n",
      " 27  screen_time_per_day            300 non-null    float64\n",
      " 28  physical_activity              299 non-null    float64\n",
      " 29  smoking_status                 299 non-null    object \n",
      " 30  average_sleep_duration_hours   300 non-null    float64\n",
      " 31  systolic                       300 non-null    float64\n",
      " 32  diastolic                      300 non-null    float64\n",
      " 33  mother_Hypertension            300 non-null    bool   \n",
      " 34  mother_No known health issues  300 non-null    bool   \n",
      " 35  mother_Type 2 diabetes         300 non-null    bool   \n",
      " 36  diet_type_Balanced             300 non-null    bool   \n",
      " 37  diet_type_Low-carb             300 non-null    bool   \n",
      " 38  diet_type_Mediterranean        300 non-null    bool   \n",
      " 39  diet_type_Other                300 non-null    bool   \n",
      " 40  diet_type_Plant-based          300 non-null    bool   \n",
      "dtypes: bool(10), float64(15), int64(4), object(12)\n",
      "memory usage: 75.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "onehot_cols = ['mother', 'diet_type']\n",
    "\n",
    "for i, col in enumerate(onehot_cols):\n",
    "    # Use onehot_cols[i] as prefix for the new columns\n",
    "    # If drop_first = False, the first category will be included in the new columns\n",
    "    # If drop_first = True, the first category will be dropped, meaning that\n",
    "    #  if all new columns are 0, the category is the first one. This is the most\n",
    "    #  efficient way of doing it, but it has the price that we have to remember that\n",
    "    # the first category is represented like this\n",
    "    df = do_dummy_encoding(df, col, onehot_cols[i], drop_first = False)\n",
    "\n",
    "# Let's take a look at the new columns\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9a176",
   "metadata": {},
   "source": [
    "In the following, we simply encode the remaining categorical features to numerical ones by label encoding.\n",
    "This naive approach introduces false order between different categories and is not in general the right approach\n",
    "when no inherent order is present between categories.\n",
    "\n",
    "#### **You are expected to encode the remaining categorical features in the best way, e.g. using the methods illustrated above.**\n",
    "\n",
    "**NB:** Make sure you remove a feature from the list below once you have found another way to encode it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adb4f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LabelEncoder to convert the remaining columns to numerical values\n",
    "labels = ['country_of_birth', 'current_city_of_residence', 'housing_type', 'housing_tenure', 'marital_status', 'sexual_orientation',\n",
    "              'father', 'specific_preferences', 'challenges', 'activities']\n",
    "labelencoder = LabelEncoder()\n",
    "for label in labels:\n",
    "    df[label] = labelencoder.fit_transform(df[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6914783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 45, 7, ..., True, False, False],\n",
       "       [1, 32, 18, ..., False, False, False],\n",
       "       [1, 28, 45, ..., False, False, False],\n",
       "       ...,\n",
       "       [1, 35, 14, ..., False, True, False],\n",
       "       [1, 32, 30, ..., False, True, False],\n",
       "       [1, 35, 57, ..., False, True, False]], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,df.columns!='patient_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "427377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having converted all features to numerical values, we can now imputate the remaining NaNs\n",
    "\n",
    "# This imputer considers all numeric features and assigns to each missing value the feature mean of the n nearest neighbors\n",
    "knn_imputer = KNNImputer(missing_values = np.nan, n_neighbors = 5)\n",
    "\n",
    "# We will use the knn imputer, since it considers all numeric features to find the most similar neighbors:\n",
    "df.loc[:,df.columns!='patient_id'] = knn_imputer.fit_transform(df.loc[:,df.columns!='patient_id'])\n",
    "\n",
    "# Finally, since non-integer number of childrens are not very meaningful, we'll round the estimated no of children\n",
    "#df['number_of_children'] = df['number_of_children'].round(decimals = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add conv ids to df\n",
    "\n",
    "df.to_csv('data/cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f7fc6",
   "metadata": {},
   "source": [
    "#### At this point, the data cleaning is completed, and we can proceed by scaling the features and potentially applying PCA analysis to reduce the total number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311840e",
   "metadata": {},
   "source": [
    "\n",
    "### Excercises:\n",
    "- There are some indications that the BMI feature contains errors. Use the height and weight features to calculate the BMI of all patients and replace the potentially wrong BMI values your results\n",
    "- Can you fig. out what the nan values in number_of_children should be (before we impute them), given that you also have access to the has_children feature? Once you do, change the nan values before imputation.\n",
    "- Just like we mapped the state_name feature onto obesity prevalences, map the country_of_origin to a metric you think might be useful for predicting type 2 diabetes. \n",
    "- Inspired by how we grouped the mother and diet_type features, group the father feature, and then move it from the label_encoding list to the one-hot encoding list\n",
    " and speficic_preferances features\n",
    "- (**To be done on your own time**) Go through the remaining features 'country_of_birth', 'current_city_of_residence', 'housing_type', 'housing_tenure', 'marital_status', 'sexual_orientation',\n",
    "               'specific_preferences', 'challenges', 'activities', and figure out the best way to encode them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
